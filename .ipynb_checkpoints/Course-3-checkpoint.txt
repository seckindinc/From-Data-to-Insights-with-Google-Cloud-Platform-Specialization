Week 1

- Approximate Aggregate Functions

    - Count(distinct) vs Approx_count_distinct()
    
- Navigation Functions

    - LEAD(): Returns the value of a row n rows ahead of the current row
    - LAG(): Returns the value of a row n rows behind the current row
    - NTH_VALUE(): returns the value of the nth value in the window

Select *, RANK() OVER ( PARTITION BY department ORDER BY start date ) AS rank
From table
WHERE rank = 1;

- User Defined Functions

    - CREATE TEMPORARY FUNCTION: Creates a new function. A function can contain zero or more named_parameters
    - RETURNS[data_type]: Specifies the data type that the function returns.
    - LANGUAGE[language]: Specifies the language for the function.
    - AS[external_code]: Specifies the code that the function runs.

CREATE TEMPORARY FUNCTION multiplyImputs(x FLOAT64, y FLOAT64)
RETURNS FLOAT64
LANGUAGE js AS """
	return x*y;
""";

WITH numbers as(
select 1 as x, 5 as y)

select multiplyImputs(8,4)
from numbers

    - Concurrent UDF function count is 6.

- Traditional Relational Database Architect

    - Adding new columns to existing schema regarding to the new business models will cause wider tables with full off null or 0 values.
    
- BigQuery Architecture 
    
    - Column-Based data storage: Rows and columns are stored in compressed format in Google Colossus. Columns are individually compressed. Access values from a few columns without reading every column.
        
    - Break apart tables into pieces: Tables are speerated into small shards and called upon to the query.BigQuery automatically pieces it all back together for queries. Shards of data are read and processed in parallel.
    
    BigQuery automatically balances and scales workers. Up to 2000 workers to process concurrent queries.
    
    Each worker is a virtual machine. 
    
             Gateway
    '                   '
    '                   '
    '                   '
    Query Master        Query Master
    Q1, Q4, Q6, Q7      Q2, Q3, Q5, Q8
    '                   '             '
    '                   '             '
    '                   '             '
    Worker              Worker        Worker
    
    BiqQuery workers communicate by shuffiling data in-memory
    
        - Workers consume data values and perform operations in parallel
        - Workers produce output to the in-memory shuffle service
        - Workers consume new data and continue processing
        
    BigQuery shuffling enables massive scale
    
        - Shuffle allows BigQuery to process massively parallel petabyte-scale data jobs
        - Everything after query execution is automatically scaled and managed
        - All queries, large and small, use shuffle
    
    - Store nested fields within a table: Rows are stored in parent - child relationships so we can store any parent with related child information in a nested way.
    
    Normalized table format;
    
    people     cities_lived
          
    name  -->  name
    age        city
    gender     years_lieved
    
    Denormalized table format;
    
    people_cities_lived
    
    name
    age
    gender
    city_name
    years_lived
    
    Repeated
    
    name
    age
    gender
    cities_lived (repeated)
        city
        years_lived
        
- Working with Repeated Fields

    - Introducing Arrays and Structs: 
    
        - Arrays are ordered list of zero or more data values that must have the same data type.
        
        create table array__
        as
        select ['a','b','c'] as array_
        
        select * from array__
        
        Row array_
        1   a
            b
            c
            
        select ARRAY_LENGTH(array_) as array_size
        
        Row array_size
        1   3
    
    - Flattening Arrays: CROSS JOIN and UNNEST flattens arrays so we can access elements
    
    select items, customer_name
    from UNNEST(['a','b','c']) as items
    CROSS JOIN
    (select 'dummy' as column_name)
    
    Row item column_name
    1   a    dummy
    2   b    dummy
    3   c    dummy
    
        - ARRAY_AGG function allows us to create arrays from rows. 
        
            select ARRAY_AGG(item) as items 
            from table

            select ARRAY_AGG(item order by item)
            from table
    
        - STRUCTs are flexible containers. 
        
            STRUCTs are a container of ordered fields each with a type (required) and field name (optional).
            
            You can store multiple data types in a STRUCT (even Arrays).
            
            One STRUCT can have many values, looks and behaves similar to a table.
            
            ARRAYS can contain STRUCTs as values
            
            select STRUCT(35 as age, 'Jacob' as name) as customer
            
            select STRUCT(35 as age, 'Jacob' as name, ['apple','pear','peach'] as items) as customer
            
            select 
            [STRUCT(35 as age, 'Jacob' as name, ['apple','pear','peach'] as items),
             STRUCT(33 as age, 'Miranda' as name, ['apple','pear','water'] as items)
            ] AS customers
            
            WITH orders as (              
                select [
                STRUCT(35 as age, 'Jacob' as name, ['apple','pear','peach'] as items),
                STRUCT(33 as age, 'Miranda' as name, ['apple','pear','water'] as items)
                ] AS customers
                )

                select customers from orders as o cross join UNNEST(o.customers) as customers where 'water' in UNNEST(customers.items)            
    
    We can use the syntax below to use nested fields without CROSS JOIN and UNNEST
    
    SELECT
      ein,
      expense
    FROM `data-to-insights.irs_990.irs_990_repeated` n, n.expense_struct AS expense
    WHERE expense.type = 'Legal'
    ORDER BY expense.amount DESC
    LIMIT 10
    
- Data Studio

    There are 2 parts to Data Studio cache: the query cache and prefetch cache

    When all the carts in the report are being served from a cache, a lightning bolt icon appears in the bottom right corner.

    Break both caches in Edit mode using Refresh cache 

    You should turn off prefect cache if your data changes frequently 

    Sharing a report does not share direct access to any added data

    Data sources must be shared separately from reports
    
Week 2

- Avoid Input / Output Wastefulness

    - Do not use 'Select *', use only the columns you need
    - Denormalize your schemas and take advantage of nested and repeated fields
    - Use granular suffixes in your table wildcards for more specificity
    
- Use BigQuery native storage for the best performance

    - External direct data connections can never be cached
    - Live edits to underlying external sources could create race conditions which is BigQuery will not know which spreadshift is the latest
    - Native BigQuery tables have intelligence built-in like automatic predicate pushdown which is when we use filters, BigQuery filters those rows beforehand and decrease the amount of data being processed
    
- Optimize communication between slots(via shuffle)

    - Pre-filter your data before doing JOINs
    - Many shuffle stages can indicate data partition issues(skew)
    
- Do not use WITH clauses in place of materializing results

    - Commonly filtering and transforming the same results? Store them into a permanent table
    - WITH clause queries are not materialized and are re-queried if referenced more than once

- Be careful using GROUP by accross many distinct values

    - Best when the number of distinct groups is small(fewer shuffles of data)
    - Grouping by high-cardinality unique ID is a bad idea
    
- Reduce Javascript UDFs to reduce computational load

   - Javascript UDFs require BigQuery to launch a Java subprocess to run
   - Use native SQL functions whenever possible
   
- Data skew

    - Filter your dataset as early as possible(this avoids overloading workers on JOINs)
    - Hint: Use the Query Explanation map and compare the Max vs the Avg times to highlight skew
    - BigQuery will automatically attempt to reshuffle workers that are overloaded with data
    
- Diagnose performance issue with Query Explanation Map

    - waitRatioAvg: Time the average worker spent waiting to be scheduled.
    - waitRatioMax: Time the slowest worker spent waiting to be scheduled.
    - readRatioAvg: Time the average worker spent reading input data.
    - readRatioMax: Time the slowest worker spent reading input data
    - computeRatioAvg: Time the average worker spent CPU-bound.
    - computeRatioMax: Time the slowest worker spent CPU-bound.
    - writeRatioAvg: Time the average worker spent writing ouput data.
    - writeRatioMax: Time the slowest worker spent writing ouput data.
    
    Explanation map shows stages of the shuffeling operations.
    
    If we only try to analyse the most recent data, data partitioning is the best option.
    
    Select 
        order_id
    from 
        table
    Where 
        _PARTITIONTIME BETWEEN T1 and T2
        
        
    