{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is BigQuery?\n",
    "\n",
    "Google BigQuery is an enterprise data warehouse built using BigTable and Google Cloud Platform. It’s serverless and completely managed.\n",
    "\n",
    "You can access BigQuery by using the Cloud Console or the classic web UI, by using a command-line tool, or by making calls to the BigQuery REST API using a variety of client libraries such as Java, .NET, or Python. There are also a variety of third-party tools that you can use to interact with BigQuery, such as visualizing the data or loading the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How BigQuery Stores Data?\n",
    "\n",
    "BigQuery stores data in a columnar format — Capacitor (which is a successor of ColumnarIO). BigQuery achieves very high compression ratio and scan throughput. Unlike ColumnarIO, now on BigQuery, you can directly operate on compressed data without decompressing it.\n",
    "Columnar storage has the following advantages:\n",
    "\n",
    "- Traffic minimization — When you submit a query, the required column values on each query are scanned and only those are transferred on query execution. E.g., a query `SELECT title FROM Collection` would access the title column values only.\n",
    "\n",
    "- Higher compression ratio — Columnar storage can achieve a compression ratio of 1:10, whereas ordinary row-based storage can compress at roughly 1:3.\n",
    "\n",
    "![title](column-oriented.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the Query Gets Executed?\n",
    "\n",
    "BigQuery depends on Borg for data processing. Borg simultaneously instantiates hundreds of Dremel jobs across required clusters made up of thousands of machines. In addition to assigning compute capacity for Dremel jobs, Borg handles fault-tolerance as well.\n",
    "\n",
    "Now, how do you design/execute a query which can run on thousands of nodes and fetches the result? This challenge was overcome by using the Tree Architecture. This architecture forms a gigantically parallel distributed tree for pushing down a query to the tree and aggregating the results from the leaves at a blazingly fast speed.\n",
    "\n",
    "![title](query-engine.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course 1\n",
    "\n",
    "#### Module 1\n",
    "\n",
    "- Reasons why Google Cloud Platform is used for Data Analysis;\n",
    "\n",
    "    - Storage is cheap\n",
    "    - Focus on queries, not infrastructure\n",
    "    - Massive scalability\n",
    "\n",
    "- Traditional big data platforms require an investment in infrastructure;\n",
    "\n",
    "    - RAM\n",
    "    - CPU\n",
    "    - Network\n",
    "    - Maintenance \n",
    "\n",
    "- Seperation of storage and computing power enables efficient resource allocation. --Pay for only the resources you are using and no more.\n",
    "\n",
    "- BigQuery scales automatically and you only pay for what you use. Fully managed infrastructure scales to process faster and you only pay bytes processes + storage.\n",
    "\n",
    "- Projects organize and govern your activities in the cloud;\n",
    "\n",
    "    - Navigate and launch cloud tools for your project by exploring the Products and Services menu\n",
    "    - Work collaboratively by adding project users through IAM(Identity and Access Management)\n",
    "    - Authorize tools and apps through the API manager\n",
    "\n",
    "- Commonly used resources by data analysts; \n",
    "\n",
    "- Storage in Google Cloud Storage\n",
    "    - Buckets are scalable containers that hold your data\n",
    "    - You can create and upload files to your buckets within your Cloud Console\n",
    "    \n",
    "- Datasets in Google BigQuery\n",
    "\n",
    "- Billed resources;\n",
    "\n",
    "    - Storage in Google Cloud Storage\n",
    "        - Billed for Bucket Storage\n",
    "\n",
    "    - Datasets in Google BigQuery\n",
    "        - Billed for Query processing\n",
    "        - Billed for Table Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Module 2\n",
    "\n",
    "- Things can be handled under GCP;\n",
    "\n",
    "    - Ingest\n",
    "    - Transform\n",
    "    - Store\n",
    "    - Analyze\n",
    "    - Visualize\n",
    "\n",
    "- In BigQuery we are billed for the bytes that we have processed so far. In this case we always need to check how much data will be processed at the specific query. On the other hand 5TB is free monthly.\n",
    "\n",
    "- Sample Query\n",
    "\n",
    "    SELECT EXTRACT(DAYOFWEEK FROM trip_start_timestamp) as day, count(1) as rides\n",
    "    FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips` \n",
    "    where trip_seconds > 1000\n",
    "    group by 1\n",
    "\n",
    "- Queries can be saved and shared via links.\n",
    "\n",
    "- Command + Click on the table allows us to visualise the columns in the table.\n",
    "\n",
    "- 9 fundamental BigQuery Features;\n",
    "\n",
    "    - Fully managed data warehouse: No-Ops, Petabyte-Scale\n",
    "    - Reliability: Backed by Google Datacenters\n",
    "    - Economical: Pay only for the processing and storage you use\n",
    "    - Security: Role ACLs, Data Encrypted in transport at rest\n",
    "    - Auditable: Every transaction logged and quaryable\n",
    "    - Scalable: Highly parallel processing model means fast queries\n",
    "    - Flexible: Mashup data across multiple datasets\n",
    "    - Easy-to-use: Familiar SQL, no indexes, open standards\n",
    "    - Public datasets: Explore and practise with real datasets\n",
    "    \n",
    "- Architecture\n",
    "    \n",
    "    User Query -> BigQuery Anaytics Engine(Job -> Analytics Engine)-> BigQuery Managed Storage(Table Name Columnar Layout)\n",
    "    \n",
    "    Google Bigquery = BigQuery Managed Storage + BigQuery Analysis(Dremel)\n",
    "    \n",
    "- In Jupyter Lab we can call '%load_ext google.cloud.bigquery' to invoke the BigQuery magic function and '%%bq query' on top of the SQL codes to directly run them in Jupyter Lab.\n",
    "\n",
    "- SQL functions;\n",
    "\n",
    "    - Formating integers;\n",
    "    \n",
    "    select FORMAT(\"%'d\",1000) \n",
    "    #Returns 1,000\n",
    "    \n",
    "    - Parsing string as date and extracting year\n",
    "    \n",
    "    SELECT PARSE_DATE('%Y%m', CAST(tax_pd as STRING)) as date,\n",
    "    EXTRACT(YEAR FROM PARSE_DATE('%Y%m', CAST(tax_pd as STRING))) as year\n",
    "    FROM `bigquery-public-data.irs_990.irs_990_2015` \n",
    "    limit 10\n",
    "    \n",
    "    - Selecting null values\n",
    "    \n",
    "    select *\n",
    "    from table\n",
    "    where column is null\n",
    "    \n",
    "- Types of jobs in BigQuery;\n",
    "    \n",
    "    - Query\n",
    "    - Load data into a table - free\n",
    "    - Extracting the data - free\n",
    "    - Copy existing table - free\n",
    "    \n",
    "- Three categories of BigQuery pricing\n",
    "\n",
    "    - Storage\n",
    "    \n",
    "        - Amount of data in table\n",
    "        - Ingest rate of streaming data\n",
    "        - Automatic discount for old data\n",
    "        \n",
    "    - Processing\n",
    "    \n",
    "        - On-demand or flat rate plans\n",
    "        - On-demand based on amount of data processed\n",
    "        - 1 TB/month free\n",
    "        - Have to opt-in to run\n",
    "        \n",
    "    - Free\n",
    "    \n",
    "        - Loading\n",
    "        - Exporting\n",
    "        - Queries on metadata\n",
    "        - Cached queries\n",
    "        - Queries with errors\n",
    "        \n",
    "- Cached table is a term that every time we run a query BigQuery stores the data in a temprorary table.\n",
    "\n",
    "- Query validator not only validates the query but also shows how much data will process when it runs.\n",
    "   \n",
    "- Quotas are used to protect all BigQuery tenants;\n",
    "\n",
    "    - 50 concurrent queries\n",
    "    - Query timeout: 6 hours\n",
    "    - 1000 updates to a table per day\n",
    "    - 1000 tables referenced by a single query\n",
    "    - Max results size: 128 MB compressed\n",
    "      \n",
    "- Optimizing queries for cost;\n",
    "\n",
    "    - Only include the columns and rows you need (filter early)\n",
    "    - Use cached results when possible\n",
    "        - Permanent tables instead of views\n",
    "        - Views are saved queries\n",
    "    - Limit the use of User-Defined Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Module 5\n",
    "\n",
    "- High quality datasets conform to strict integrity rules;\n",
    "\n",
    "    - Validity\n",
    "    \n",
    "        - Out of range values\n",
    "        - Empty fields\n",
    "        - Data mismatch which means a certain value does not match\n",
    "    \n",
    "    - Accuracy \n",
    "    \n",
    "        - Lookup datasets\n",
    "\n",
    "    - Completeness\n",
    "    \n",
    "        - Missing data\n",
    "        \n",
    "    - Consistency\n",
    "    \n",
    "        - Duplicate records\n",
    "        - Concurrency issues\n",
    "        \n",
    "    - Uniformity\n",
    "    \n",
    "        - Same units of measurement\n",
    "        \n",
    "- Cloud Data Prep\n",
    "\n",
    "    - Flow based ETL. Uses predefined 'wranglers' (aggregate, deduplicate etc) to transform the data.\n",
    "    - Chain transformation rules, wranglers, together into a 'recipe'.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course 2\n",
    "\n",
    "#### Week 1 \n",
    "\n",
    "- Temprorary or Permanent Table\n",
    "\n",
    "    - All Query Results are saved to either a Temprorary or Permanent Table\n",
    "    - If you specify a Destination Table then that table becomes Permanent, otherwise it is a new Temprorary Table\n",
    "    - Temprorary Tables are the basis of Query Cached Results\n",
    "    - Temprorary Tables last 24 hours only\n",
    "    \n",
    "- Cache\n",
    "\n",
    "    - Cache = Faster Results. Cache is selected by default.\n",
    "    - Cache is not used when;\n",
    "    \n",
    "        - Underlying table(s) updated\n",
    "        - Deterministic queries used(like CURRENT_TIMESTAMP())\n",
    "        - Cache disabled in Show Options\n",
    "        \n",
    "- Storing results in a View\n",
    "\n",
    "    - View = Saved SQL Query(a virtual table)\n",
    "    - The underlying query is re-ran each time the view is queried"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Week 2\n",
    "\n",
    "- We can ingest data permanently into BigQuery from a variety of formats;\n",
    "\n",
    "    - Cloud Storage\n",
    "    - Google Drive\n",
    "    - Cloud Dataprep\n",
    "    - Cloud BigTable\n",
    "    - Csv, Json, Avro\n",
    "    \n",
    "- External table is like a pointer to the file. It is not stored in BigQuery managed storage and we can't benefit from the permanent table perks, like cache.\n",
    "\n",
    "- Partitioned tables\n",
    "\n",
    "    A partitioned table is a special table that is divided into segments, called partitions, that make it easier to manage and query your data. By dividing a large table into smaller partitions, you can improve query performance, and you can control costs by reducing the number of bytes read by a query.\n",
    "\n",
    "    - There are two types of table partitioning in BigQuery:\n",
    "\n",
    "        - Tables partitioned by ingestion time: Tables partitioned based on the data's ingestion (load) date or arrival date.\n",
    "        - Partitioned tables: Tables that are partitioned based on a TIMESTAMP or DATE column.\n",
    "    \n",
    "    Tables without partitions need to be scanned for all the records to see whether they satisfy the date-matching condition in the Where clause. \n",
    "\n",
    "    Query below creates a partitioned table with the partition column date. date column was created in the select statement. \n",
    "\n",
    "    #standardSQL\n",
    "     CREATE OR REPLACE TABLE ecommerce.days_with_rain\n",
    "     PARTITION BY date\n",
    "     OPTIONS (\n",
    "       partition_expiration_days=90,\n",
    "       description=\"weather stations with precipitation, partitioned by day\"\n",
    "     ) AS\n",
    "     SELECT\n",
    "       DATE(CAST(year AS INT64), CAST(mo AS INT64), CAST(da AS INT64)) AS date,\n",
    "       (SELECT ANY_VALUE(name) FROM `bigquery-public-data.noaa_gsod.stations` AS stations\n",
    "        WHERE stations.usaf = stn) AS station_name,  -- Stations may have multiple names\n",
    "       prcp\n",
    "     FROM `bigquery-public-data.noaa_gsod.gsod*` AS weather\n",
    "     WHERE prcp < 99.9  -- Filter unknown values\n",
    "       AND prcp > 0      -- Filter stations/days with no precipitation\n",
    "       AND _TABLE_SUFFIX = CAST( EXTRACT(YEAR FROM CURRENT_DATE()) AS STRING)\n",
    "  \n",
    "- BigQuery Table Wildcards\n",
    "\n",
    "    When there are lots of table needs to be merged with union method it is not feasible to union one by one. If the table names have a suffix or prefix pattern we can query them with wildcard operators. \n",
    "    \n",
    "    - Query below selects all columns from the tables start with gsod name;\n",
    "    \n",
    "        select *\n",
    "        from `bigquery-public-data.noaa_gsod.gsod*`\n",
    "        \n",
    "    - Filtering tables with a certain pattern of suffix;\n",
    "    \n",
    "        select *, _TABLE_SUFFIX as table_year\n",
    "        from `bigquery-public-data.noaa_gsod.gsod*`\n",
    "        where _TABLE_SUFFIX > '1950'\n",
    "\n",
    "- Table Joins\n",
    "\n",
    "    - We can do joins between a table and bulk of tables which were selected with wildcards.\n",
    "    - Joining on Non-Unique fields explode your dataset         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Week 3\n",
    "\n",
    "- Dimensions and Measures\n",
    "\n",
    "    - Dimensions\n",
    "    \n",
    "        A field that can be considered an independent variable. Usually contains qualitative, categorical information.\n",
    "        \n",
    "        Examples;\n",
    "        \n",
    "            - Name\n",
    "            - Location\n",
    "            - Job title\n",
    "        \n",
    "    - Measures\n",
    "    \n",
    "        A field that is a dependent variable; that is, the value is a function of one or more dimensions. e.g. any field contains numeric (quantitive) information.\n",
    "        \n",
    "        Examples; \n",
    "        \n",
    "            - Revenue\n",
    "            - Salary\n",
    "            - Expenses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course 3\n",
    "\n",
    "#### Week 1\n",
    "\n",
    "- Approximate Aggregate Functions\n",
    "\n",
    "    - Count(distinct) vs Approx_count_distinct()\n",
    "    \n",
    "- Navigation Functions\n",
    "\n",
    "    - LEAD(): Returns the value of a row n rows ahead of the current row\n",
    "    - LAG(): Returns the value of a row n rows behind the current row\n",
    "    - NTH_VALUE(): returns the value of the nth value in the window\n",
    "\n",
    "Select *, RANK() OVER ( PARTITION BY department ORDER BY start date ) AS rank\n",
    "From table\n",
    "WHERE rank = 1;\n",
    "\n",
    "- User Defined Functions\n",
    "\n",
    "    - CREATE TEMPORARY FUNCTION: Creates a new function. A function can contain zero or more named_parameters\n",
    "    - RETURNS[data_type]: Specifies the data type that the function returns.\n",
    "    - LANGUAGE[language]: Specifies the language for the function.\n",
    "    - AS[external_code]: Specifies the code that the function runs.\n",
    "\n",
    "CREATE TEMPORARY FUNCTION multiplyImputs(x FLOAT64, y FLOAT64)\n",
    "RETURNS FLOAT64\n",
    "LANGUAGE js AS \"\"\"\n",
    "\treturn x*y;\n",
    "\"\"\";\n",
    "\n",
    "WITH numbers as(\n",
    "select 1 as x, 5 as y)\n",
    "\n",
    "select multiplyImputs(8,4)\n",
    "from numbers\n",
    "\n",
    "    - Concurrent UDF function count is 6.\n",
    "\n",
    "- Traditional Relational Database Architect\n",
    "\n",
    "    - Adding new columns to existing schema regarding to the new business models will cause wider tables with full off null or 0 values.\n",
    "    \n",
    "- BigQuery Architecture \n",
    "    \n",
    "    - Column-Based data storage: Rows and columns are stored in compressed format in Google Colossus. Columns are individually compressed. Access values from a few columns without reading every column.\n",
    "        \n",
    "    - Break apart tables into pieces: Tables are speerated into small shards and called upon to the query. BigQuery automatically pieces it all back together for queries. Shards of data are read and processed in parallel.\n",
    "    \n",
    "    BigQuery automatically balances and scales workers. Up to 2000 workers to process concurrent queries.\n",
    "    \n",
    "    Each worker is a virtual machine. \n",
    "    \n",
    "             Gateway\n",
    "    '                   '\n",
    "    '                   '\n",
    "    '                   '\n",
    "    Query Master        Query Master\n",
    "    Q1, Q4, Q6, Q7      Q2, Q3, Q5, Q8\n",
    "    '                   '             '\n",
    "    '                   '             '\n",
    "    '                   '             '\n",
    "    Worker              Worker        Worker\n",
    "    \n",
    "    BiqQuery workers communicate by shuffiling data in-memory\n",
    "    \n",
    "        - Workers consume data values and perform operations in parallel\n",
    "        - Workers produce output to the in-memory shuffle service\n",
    "        - Workers consume new data and continue processing\n",
    "        \n",
    "    BigQuery shuffling enables massive scale\n",
    "    \n",
    "        - Shuffle allows BigQuery to process massively parallel petabyte-scale data jobs\n",
    "        - Everything after query execution is automatically scaled and managed\n",
    "        - All queries, large and small, use shuffle\n",
    "    \n",
    "    - Store nested fields within a table: Rows are stored in parent - child relationships so we can store any parent with related child information in a nested way.\n",
    "    \n",
    "    Normalized table format;\n",
    "    \n",
    "    people     cities_lived\n",
    "          \n",
    "    name  -->  name\n",
    "    \n",
    "    age        city\n",
    "    \n",
    "    gender     years_lieved\n",
    "    \n",
    "    Denormalized table format;\n",
    "    \n",
    "    people_cities_lived\n",
    "    \n",
    "    name\n",
    "    \n",
    "    age\n",
    "    \n",
    "    gender\n",
    "    \n",
    "    city_name\n",
    "    \n",
    "    years_lived\n",
    "    \n",
    "    Repeated\n",
    "    \n",
    "    name\n",
    "    \n",
    "    age\n",
    "    \n",
    "    gender\n",
    "    \n",
    "    cities_lived (repeated)\n",
    "    \n",
    "        city\n",
    "        \n",
    "        years_lived\n",
    "        \n",
    "- Working with Repeated Fields\n",
    "\n",
    "    - Introducing Arrays and Structs: \n",
    "    \n",
    "        - Arrays are ordered list of zero or more data values that must have the same data type.\n",
    "        \n",
    "        create table array__\n",
    "        as\n",
    "        select ['a','b','c'] as array_\n",
    "        \n",
    "        select * from array__\n",
    "        \n",
    "        array_\n",
    "            a\n",
    "            b\n",
    "            c\n",
    "            \n",
    "        select ARRAY_LENGTH(array_) as array_size\n",
    "        \n",
    "        array_size\n",
    "            3\n",
    "    \n",
    "    - Flattening Arrays: CROSS JOIN and UNNEST flattens arrays so we can access elements\n",
    "    \n",
    "    select items, customer_name\n",
    "    from UNNEST(['a','b','c']) as items\n",
    "    CROSS JOIN\n",
    "    (select 'dummy' as column_name)\n",
    "    \n",
    "    Row item column_name\n",
    "    1   a    dummy\n",
    "    2   b    dummy\n",
    "    3   c    dummy\n",
    "    \n",
    "        - ARRAY_AGG function allows us to create arrays from rows. \n",
    "        \n",
    "            select ARRAY_AGG(item) as items \n",
    "            from table\n",
    "\n",
    "            select ARRAY_AGG(item order by item)\n",
    "            from table\n",
    "    \n",
    "        - STRUCTs are flexible containers. \n",
    "        \n",
    "            STRUCTs are a container of ordered fields each with a type (required) and field name (optional).\n",
    "            \n",
    "            You can store multiple data types in a STRUCT (even Arrays).\n",
    "            \n",
    "            One STRUCT can have many values, looks and behaves similar to a table.\n",
    "            \n",
    "            ARRAYS can contain STRUCTs as values\n",
    "            \n",
    "            select STRUCT(35 as age, 'Jacob' as name) as customer\n",
    "            \n",
    "            select STRUCT(35 as age, 'Jacob' as name, ['apple','pear','peach'] as items) as customer\n",
    "            \n",
    "            select \n",
    "            [STRUCT(35 as age, 'Jacob' as name, ['apple','pear','peach'] as items),\n",
    "             STRUCT(33 as age, 'Miranda' as name, ['apple','pear','water'] as items)\n",
    "            ] AS customers\n",
    "            \n",
    "            WITH orders as (              \n",
    "                select [\n",
    "                STRUCT(35 as age, 'Jacob' as name, ['apple','pear','peach'] as items),\n",
    "                STRUCT(33 as age, 'Miranda' as name, ['apple','pear','water'] as items)\n",
    "                ] AS customers\n",
    "                )\n",
    "\n",
    "                select customers from orders as o cross join UNNEST(o.customers) as customers where 'water' in UNNEST(customers.items)            \n",
    "    \n",
    "    We can use the syntax below to use nested fields without CROSS JOIN and UNNEST\n",
    "    \n",
    "    SELECT\n",
    "      ein,\n",
    "      expense\n",
    "    FROM `data-to-insights.irs_990.irs_990_repeated` n, n.expense_struct AS expense\n",
    "    WHERE expense.type = 'Legal'\n",
    "    ORDER BY expense.amount DESC\n",
    "    LIMIT 10\n",
    "    \n",
    "- Data Studio\n",
    "\n",
    "    There are 2 parts to Data Studio cache: the query cache and prefetch cache\n",
    "\n",
    "    When all the carts in the report are being served from a cache, a lightning bolt icon appears in the bottom right corner.\n",
    "\n",
    "    Break both caches in Edit mode using Refresh cache \n",
    "\n",
    "    You should turn off prefect cache if your data changes frequently \n",
    "\n",
    "    Sharing a report does not share direct access to any added data\n",
    "\n",
    "    Data sources must be shared separately from reports   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Week 2\n",
    "\n",
    "- Avoid Input / Output Wastefulness\n",
    "\n",
    "    - Do not use 'Select *', use only the columns you need\n",
    "    - Denormalize your schemas and take advantage of nested and repeated fields\n",
    "    - Use granular suffixes in your table wildcards for more specificity\n",
    "    \n",
    "- Use BigQuery native storage for the best performance\n",
    "\n",
    "    - External direct data connections can never be cached\n",
    "    - Live edits to underlying external sources could create race conditions which is BigQuery will not know which spreadsheat is the latest\n",
    "    - Native BigQuery tables have intelligence built-in like automatic predicate pushdown which is when we use filters, BigQuery filters those rows beforehand and decrease the amount of data being processed\n",
    "    \n",
    "- Optimize communication between slots(via shuffle)\n",
    "\n",
    "    - Pre-filter your data before doing JOINs\n",
    "    - Many shuffle stages can indicate data partition issues(skew)\n",
    "    \n",
    "- Do not use WITH clauses in place of materializing results\n",
    "\n",
    "    - Commonly filtering and transforming the same results? Store them into a permanent table\n",
    "    - WITH clause queries are not materialized and are re-queried if referenced more than once\n",
    "\n",
    "- Be careful using GROUP by accross many distinct values\n",
    "\n",
    "    - Best when the number of distinct groups is small(fewer shuffles of data)\n",
    "    - Grouping by high-cardinality unique ID is a bad idea\n",
    "    \n",
    "- Reduce Javascript UDFs to reduce computational load\n",
    "\n",
    "   - Javascript UDFs require BigQuery to launch a Java subprocess to run\n",
    "   - Use native SQL functions whenever possible\n",
    "   \n",
    "- Data skew\n",
    "\n",
    "    - Filter your dataset as early as possible(this avoids overloading workers on JOINs)\n",
    "    - Hint: Use the Query Explanation map and compare the Max vs the Avg times to highlight skew\n",
    "    - BigQuery will automatically attempt to reshuffle workers that are overloaded with data\n",
    "    \n",
    "- Diagnose performance issue with Query Explanation Map\n",
    "\n",
    "    - waitRatioAvg: Time the average worker spent waiting to be scheduled.\n",
    "    - waitRatioMax: Time the slowest worker spent waiting to be scheduled.\n",
    "    - readRatioAvg: Time the average worker spent reading input data.\n",
    "    - readRatioMax: Time the slowest worker spent reading input data\n",
    "    - computeRatioAvg: Time the average worker spent CPU-bound.\n",
    "    - computeRatioMax: Time the slowest worker spent CPU-bound.\n",
    "    - writeRatioAvg: Time the average worker spent writing ouput data.\n",
    "    - writeRatioMax: Time the slowest worker spent writing ouput data.\n",
    "    \n",
    "    Explanation map shows stages of the shuffeling operations.\n",
    "    \n",
    "    If we only try to analyse the most recent data, data partitioning is the best option.\n",
    "    \n",
    "    Select \n",
    "        order_id\n",
    "    from \n",
    "        table\n",
    "    Where \n",
    "        _PARTITIONTIME BETWEEN T1 and T2\n",
    "        \n",
    "- Authorization\n",
    "\n",
    "    Permissions are set at dataset level not table level. We can grant access to datasets or create authorised views and grant to them.\n",
    "\n",
    "    When a project is created, BigQuery grants the Owner role to the user who created the project. Viewer, editor and owner are the primitive roles.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course 4\n",
    "\n",
    "#### Week 1\n",
    "\n",
    "- Cloud Vision API provides pretrained models to predict and serves the API as a service.\n",
    "\n",
    "- Cloud Translate API provides ptrained models to convert texts between languages or sentimental analysis.\n",
    "\n",
    "- Google Cloud Datalab\n",
    "\n",
    "  Developer Laptop --> Notebook Cloud DataLab <--Hosted on-- Compute Engine\n",
    "                               |\n",
    "  Users --> Notebook files Cloud repository\n",
    "  \n",
    "  Using '-n' to provide a name for the query as you can have more than one query in a single notebook.\n",
    "  \n",
    "  BigQuery operations have defined parameters;\n",
    "     \n",
    "     - %%bq datasets\n",
    "     - %%bq tables\n",
    "     - %%bq query\n",
    "  \n",
    "#### Week 2\n",
    "\n",
    "    - Good dataset feature columns must be;\n",
    "        \n",
    "        - Related to the objective\n",
    "        - Known at prediction-time\n",
    "        - Numeric with meaningful magnitude\n",
    "        - Have enough examples\n",
    "        - Bring human insight to problem\n",
    "        \n",
    "    - Tools to creating data pipelines;\n",
    "    \n",
    "        - Dataprep(batch)\n",
    "        - Dataflow(batch/stream)\n",
    "        - Cloud Composer\n",
    "       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
